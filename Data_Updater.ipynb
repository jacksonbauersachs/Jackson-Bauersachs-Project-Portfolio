{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85493143-6479-4726-90b5-4f0c3478edc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver                                            \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.baseball-reference.com/leagues/MLB-schedule.shtml' \n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')  \n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "def save_fully_loaded_html(box_url, save_path):\n",
    "    driver.get(box_url)\n",
    "    try:\n",
    "        WebDriverWait(driver, 5, poll_frequency=0.1).until(EC.presence_of_element_located((By.ID, \"content\")))\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: {e}\")\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(driver.page_source)\n",
    "\n",
    "def get_box_score_links(main_url):\n",
    "    driver.get(main_url)\n",
    "    WebDriverWait(driver, 5, poll_frequency=0.1).until(EC.presence_of_element_located((By.ID, \"content\")))\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    links = []\n",
    "    for link in soup.find_all('a', string='Boxscore'):\n",
    "        href = link.get('href')\n",
    "        if href and href.startswith('/boxes/'):\n",
    "            links.append('https://www.baseball-reference.com' + href)\n",
    "    return links\n",
    "\n",
    "def scrape_box_scores(box_score_links):\n",
    "    total_links = len(box_score_links)\n",
    "    print(f\"Total box scores to scrape: {total_links}\")\n",
    "\n",
    "    for i, link in enumerate(box_score_links):\n",
    "        file_name = link.split('/')[-1] + '.html'\n",
    "        save_path = os.path.join(save_dir, file_name)\n",
    "        \n",
    "        if os.path.exists(save_path):\n",
    "            print(f\"{i+1}/{total_links}: {file_name} already exists, skipping.\")\n",
    "            continue\n",
    "\n",
    "        success = False\n",
    "        delay = 300  \n",
    "        while not success:\n",
    "            try:\n",
    "                save_fully_loaded_html(link, save_path)\n",
    "                print(f\"{i+1}/{total_links}: Saved {file_name}\")\n",
    "                success = True\n",
    "                time.sleep(random.uniform(1, 2))\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving {file_name}: {e}\")\n",
    "                print(f\"Retrying in {delay // 60} minutes...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2 \n",
    "\n",
    "box_score_links = get_box_score_links(url)\n",
    "scrape_box_scores(box_score_links)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a9a5a0-8c76-4308-923f-72d098f4ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import re\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "dir_path = 'Box_Scores_New'\n",
    "box_scores = [file for file in os.listdir(dir_path) if not file.startswith('.') and file.endswith('.html')]\n",
    "file_paths = [os.path.join(dir_path, file) for file in box_scores]\n",
    "\n",
    "consolidated_file_path = 'New_Box_Scores.csv'\n",
    "if os.path.exists(consolidated_file_path):\n",
    "    consolidated_df = pd.read_csv(consolidated_file_path)\n",
    "    processed_files = set(consolidated_df['Source_File'].unique())\n",
    "else:\n",
    "    consolidated_df = pd.DataFrame()\n",
    "    processed_files = set()\n",
    "\n",
    "def parse_html(box_score):\n",
    "    with open(box_score, 'r', encoding='utf-8') as f:\n",
    "        html = f.read()\n",
    "    return BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "def read_line_score(soup):\n",
    "    linescore_table = soup.find(\"table\", {\"class\": \"linescore\"})\n",
    "    if linescore_table:\n",
    "        table_str = str(linescore_table)\n",
    "        tables = pd.read_html(StringIO(table_str))\n",
    "        if tables:\n",
    "            line_score = tables[0]\n",
    "            line_score_filtered = line_score[line_score.apply(lambda row: str(row.iloc[-3]).isdigit() and str(row.iloc[-2]).isdigit() and str(row.iloc[-1]).isdigit(), axis=1)]\n",
    "            line_score_filtered = line_score_filtered.iloc[:, [1, -3, -2, -1]]\n",
    "            line_score_filtered.columns = [\"Team\", \"TR\", \"TH\", \"TE\"]\n",
    "            return line_score_filtered\n",
    "    return None\n",
    "\n",
    "def extract_position(df, name_column='Name'):\n",
    "    positions = {'C', '1B', '2B', 'SS', '3B', 'LF', 'RF', 'CF', 'P', 'DH', 'PR', 'PH'}\n",
    "    position_regex = '|'.join(positions)\n",
    "    df[['Name', 'Position']] = df[name_column].str.extract(r'^(.*)\\s+((?:{})-(?:{})-(?:{})|(?:{})-(?:{})|(?:{}))$'.format(position_regex, position_regex, position_regex, position_regex, position_regex, position_regex), expand=True)\n",
    "    df['Position'] = df['Position'].replace('Unknown', np.nan)\n",
    "    return df\n",
    "\n",
    "def extract_detail_columns(df, column_name):\n",
    "    unique_details = set()\n",
    "    for details in df[column_name].dropna():\n",
    "        detail_items = details.split(',')\n",
    "        for item in detail_items:\n",
    "                        unique_details.add(item.strip())\n",
    "    for detail in unique_details:\n",
    "        df[detail] = 0\n",
    "\n",
    "    def populate_details(row):\n",
    "        if pd.isna(row[column_name]): return row\n",
    "        detail_items = row[column_name].split(',')\n",
    "        for item in detail_items:\n",
    "            detail = item.strip()\n",
    "            if detail in df.columns:\n",
    "                row[detail] += 1\n",
    "        return row\n",
    "\n",
    "    df = df.apply(populate_details, axis=1)\n",
    "    df.drop(columns=[column_name], inplace=True)\n",
    "    return df\n",
    "\n",
    "def read_batting_away(soup, away_team):\n",
    "    batting_divs = soup.find_all(\"div\", {\"id\": lambda x: x and x.endswith(\"batting\")})\n",
    "    if len(batting_divs) > 0:\n",
    "        batting_table = batting_divs[0].find(\"table\")\n",
    "        if batting_table:\n",
    "            table_str = str(batting_table)\n",
    "            tables = pd.read_html(StringIO(table_str))\n",
    "            if tables:\n",
    "                batting = tables[0]\n",
    "                batting = batting.dropna(subset=['Batting'])\n",
    "                batting_cleaned = batting.rename(columns={'Batting': 'Name'}, inplace=False)\n",
    "                batting_cleaned = extract_position(batting_cleaned, 'Name')\n",
    "                batting_cleaned = extract_detail_columns(batting_cleaned, 'Details')\n",
    "                if batting_cleaned.index[-1] == batting_cleaned.index[-1]:  \n",
    "                    batting_cleaned.loc[batting_cleaned.index[-1], 'Name'] = f'{away_team} Batting Totals'\n",
    "                return batting_cleaned\n",
    "    return None\n",
    "\n",
    "def read_batting_home(soup, home_team, line_score):\n",
    "    team_name_batting = home_team + 'batting'\n",
    "    tnbns = team_name_batting.replace(\" \", \"\")\n",
    "    batting_divs = soup.find_all(\"div\", {\"id\": lambda x: x and x.endswith(tnbns)})\n",
    "    if len(batting_divs) > 1:\n",
    "        batting_table = batting_divs[1].find(\"table\")\n",
    "        if batting_table:\n",
    "            table_str = str(batting_table)\n",
    "            tables = pd.read_html(StringIO(table_str))\n",
    "            if tables:\n",
    "                batting = tables[0]\n",
    "                batting = batting.dropna(subset=['Batting'])\n",
    "                batting_cleaned = batting.rename(columns={'Batting': 'Name'}, inplace=False)\n",
    "                batting_cleaned = extract_position(batting_cleaned, 'Name')\n",
    "                batting_cleaned = extract_detail_columns(batting_cleaned, 'Details')\n",
    "                if batting_cleaned.index[-1] == batting_cleaned.index[-1]:  \n",
    "                    batting_cleaned.loc[batting_cleaned.index[-1], 'Name'] = f'{home_team} Batting Totals'\n",
    "                return batting_cleaned\n",
    "    return None\n",
    "\n",
    "\n",
    "def read_pitching_away(soup, away_team):\n",
    "    pitching_divs = soup.find_all(\"div\", {\"id\": lambda x: x and x.endswith(\"pitching\")})\n",
    "    if len(pitching_divs) > 0:\n",
    "        pitching_table = pitching_divs[0].find(\"table\")\n",
    "        if pitching_table:\n",
    "            table_str = str(pitching_table)\n",
    "            tables = pd.read_html(StringIO(table_str))\n",
    "            if tables:\n",
    "                pitching = tables[0]\n",
    "                pitching_cleaned = pitching.dropna(subset=['Pitching'])\n",
    "                pitching_cleaned['Pitching'] = pitching_cleaned['Pitching'].apply(lambda x: x.split(',')[0])\n",
    "                pitching_cleaned2 = pitching_cleaned.rename(columns={'Pitching': 'Name', 'R': 'R-A', 'H': 'H-A', 'BB': 'BB-A', 'SO': 'SO-A', 'Str': 'Str-A', 'WPA': 'WPA-A', 'cWPA': 'CWPA-A', 'acLI': 'acLI-A', 'RE24': 'RE24-A', 'aLI': 'aLI-A', 'HR': 'HR-A', 'Pit': 'Pit-A'}, inplace=False)\n",
    "                pitching_cleaned2['Position'] = 'P'\n",
    "                if pitching_cleaned2.index[-1] == pitching_cleaned2.index[-1]:  \n",
    "                    pitching_cleaned2.loc[pitching_cleaned2.index[-1], 'Name'] = f'{away_team} Pitching Totals'\n",
    "                return pitching_cleaned2\n",
    "    return None\n",
    "\n",
    "def read_pitching_home(soup, home_team, line_score):\n",
    "    team_name_pitching = home_team + 'pitching'\n",
    "    tnpns = team_name_pitching.replace(\" \", \"\")\n",
    "    pitching_divs = soup.find_all(\"div\", {\"id\": lambda x: x and x.endswith(tnpns)})\n",
    "    if len(pitching_divs) > 1:\n",
    "        pitching_table = pitching_divs[1].find(\"table\")\n",
    "        if pitching_table:\n",
    "            table_str = str(pitching_table)\n",
    "            tables = pd.read_html(StringIO(table_str))\n",
    "            if tables:\n",
    "                pitching = tables[0]\n",
    "                pitching_cleaned = pitching.dropna(subset=['Pitching'])\n",
    "                pitching_cleaned['Pitching'] = pitching_cleaned['Pitching'].apply(lambda x: x.split(',')[0])\n",
    "                pitching_cleaned2 = pitching_cleaned.rename({'Pitching': 'Name', 'R': 'R-A', 'H': 'H-A', 'BB': 'BB-A', 'SO': 'SO-A', 'Str': 'Str-A', 'WPA': 'WPA-A', 'cWPA': 'CWPA-A', 'acLI': 'acLI-A', 'RE24': 'RE24-A', 'aLI': 'aLI-A', 'HR': 'HR-A', 'Pit': 'Pit-A'}, axis=1)\n",
    "                pitching_cleaned2['Position'] = 'P'\n",
    "                if pitching_cleaned2.index[-1] == pitching_cleaned2.index[-1]: \n",
    "                    pitching_cleaned2.loc[pitching_cleaned2.index[-1], 'Name'] = f'{home_team} Pitching Totals'\n",
    "                return pitching_cleaned2\n",
    "    return None\n",
    "    \n",
    "def read_game_info(soup):\n",
    "    teams = [team.get_text() for team in soup.select('strong a')]\n",
    "    scores = [score.get_text() for score in soup.select('div.scores div.score')]\n",
    "    date = soup.select_one('.scorebox_meta div:nth-of-type(1)').get_text()\n",
    "    start_time = soup.select_one('.scorebox_meta div:nth-of-type(2)').get_text().replace(\"Start Time: \", \"\")\n",
    "    attendance = soup.select_one('.scorebox_meta div:nth-of-type(3)').get_text().replace(\"Attendance: \", \"\")\n",
    "    venue = soup.select_one('.scorebox_meta div:nth-of-type(4)').get_text().replace(\"Venue: \", \"\")\n",
    "    duration = soup.select_one('.scorebox_meta div:nth-of-type(5)').get_text().replace(\"Duration: \", \"\")\n",
    "    conditions = soup.select_one('.scorebox_meta div:nth-of-type(6)').get_text()\n",
    "    data = {'Date': date,\n",
    "        'Start Time': start_time,\n",
    "        'Attendance': attendance,\n",
    "        'Venue': venue,\n",
    "        'Duration': duration,\n",
    "        'Conditions': conditions\n",
    "    }\n",
    "    return pd.DataFrame([data])\n",
    "\n",
    "def process_file(file_path):\n",
    "    try:\n",
    "        soup = parse_html(file_path)\n",
    "        line_score = read_line_score(soup)\n",
    "        if line_score is not None:\n",
    "            away_team = line_score.iloc[0, 0]\n",
    "            home_team = line_score.iloc[1, 0]\n",
    "            batting_away = read_batting_away(soup, away_team)\n",
    "            batting_home = read_batting_home(soup, home_team, line_score)\n",
    "            pitching_away = read_pitching_away(soup, away_team)\n",
    "            pitching_home = read_pitching_home(soup, home_team, line_score)\n",
    "            game_info = read_game_info(soup)\n",
    "\n",
    "        away_merged = batting_away.merge(pitching_away, on='Name', how='outer', suffixes=('', '_Pitching'))\n",
    "        home_merged = batting_home.merge(pitching_home, on='Name', how='outer', suffixes=('', '_Pitching'))\n",
    "\n",
    "        if 'Position' in away_merged.columns and 'Position_Pitching' in away_merged.columns:\n",
    "            away_merged['Position'] = away_merged['Position'].combine_first(away_merged['Position_Pitching'])\n",
    "            away_merged.drop(columns=['Position_Pitching'], inplace=True)\n",
    "\n",
    "        if 'Position' in home_merged.columns and 'Position_Pitching' in home_merged.columns:\n",
    "            home_merged['Position'] = home_merged['Position'].combine_first(home_merged['Position_Pitching'])\n",
    "            home_merged.drop(columns=['Position_Pitching'], inplace=True)\n",
    "\n",
    "        if line_score is not None and not line_score.empty:\n",
    "            for col in line_score.columns:\n",
    "                away_merged[col] = line_score.iloc[0][col]\n",
    "                home_merged[col] = line_score.iloc[1][col]\n",
    "\n",
    "        home_merged['Home/Away'] = 1\n",
    "        away_merged['Home/Away'] = 0\n",
    "        away_opposing_team = home_merged.loc[0, 'Team'] \n",
    "        home_opposing_team = away_merged.loc[0, 'Team'] \n",
    "        away_merged['Opposing Team'] = away_opposing_team\n",
    "        home_merged['Opposing Team'] = home_opposing_team\n",
    "        away_opposing_pitcher = pitching_home.loc[0, 'Name']\n",
    "        home_opposing_pitcher = pitching_away.loc[0, 'Name']\n",
    "        away_merged['Opposing Pitcher'] = away_opposing_pitcher\n",
    "        home_merged['Opposing Pitcher'] = home_opposing_pitcher\n",
    "        home_runs = line_score.loc[1, 'TR']\n",
    "        away_runs = line_score.loc[0, 'TR']\n",
    "        if home_runs > away_runs: \n",
    "            home_merged['W/L'] = 1\n",
    "            away_merged['W/L'] = 0\n",
    "        else:\n",
    "             home_merged['W/L'] = 0\n",
    "             away_merged['W/L'] = 1\n",
    "\n",
    "        all_merged = away_merged.merge(home_merged, on= ['Name', 'W/L', 'Opposing Pitcher', 'Opposing Team', 'Home/Away'], how='outer', suffixes=('_away', '_home'))\n",
    "        for col in batting_away.columns.union(batting_home.columns).union(pitching_away.columns).union(pitching_home.columns):\n",
    "            if col == 'Name':\n",
    "                continue\n",
    "            away_col = f'{col}_away'\n",
    "            home_col = f'{col}_home'\n",
    "            if away_col in all_merged.columns and home_col in all_merged.columns:\n",
    "                all_merged[col] = all_merged[away_col].combine_first(all_merged[home_col])\n",
    "                all_merged.drop(columns=[away_col, home_col], inplace=True)\n",
    "            elif away_col in all_merged.columns:\n",
    "                all_merged.rename(columns={away_col: col}, inplace=True)\n",
    "            elif home_col in all_merged.columns:\n",
    "                all_merged.rename(columns={home_col: col}, inplace=True)\n",
    "\n",
    "        specific_columns = ['Team', 'TR', 'TH', 'TE']\n",
    "        for col in specific_columns:\n",
    "            away_col = f'{col}_away'\n",
    "            home_col = f'{col}_home'\n",
    "            if away_col in all_merged.columns and home_col in all_merged.columns:\n",
    "                all_merged[col] = all_merged[away_col].combine_first(all_merged[home_col])\n",
    "                all_merged.drop(columns=[away_col, home_col], inplace=True)\n",
    "            elif away_col in all_merged.columns:\n",
    "                all_merged.rename(columns={away_col: col}, inplace=True)\n",
    "            elif home_col in all_merged.columns:\n",
    "                all_merged.rename(columns={home_col: col}, inplace=True)\n",
    "\n",
    "        if game_info is not None and not game_info.empty:\n",
    "            for col in game_info.columns:\n",
    "                all_merged[col] = game_info.iloc[0][col]\n",
    "\n",
    "        def convert_percentages_to_decimals(df, columns):\n",
    "            for column in columns:\n",
    "                df[column] = df[column].str.replace('%', '').astype(float) / 100\n",
    "            return df\n",
    "\n",
    "        percentage_columns = ['CWPA-A', 'cWPA']\n",
    "        df = convert_percentages_to_decimals(all_merged, percentage_columns)\n",
    "\n",
    "        all_merged.fillna(0, inplace=True)\n",
    "        all_merged.replace('*', 0, inplace=True)\n",
    "        all_merged = all_merged[all_merged['Name'] != '0']\n",
    "        all_merged['Date'] = pd.to_datetime(all_merged['Date'], format='%A, %B %d, %Y').dt.strftime('%Y-%m-%d')\n",
    "        all_merged['H+R+RBI'] = all_merged[['H', 'R', 'RBI']].sum(axis=1)\n",
    "        Date = all_merged.loc[0, 'Date']\n",
    "        Year = Date[:4]\n",
    "        Month = Date[5:7]\n",
    "        Day = Date[8:]\n",
    "        all_merged['Year'] = Year\n",
    "        all_merged['Month'] = Month\n",
    "        all_merged['Day'] = Day\n",
    "        date_str = all_merged.loc[0, 'Date']\n",
    "        date_obj = datetime.datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        day_of_week = date_obj.strftime(\"%A\")\n",
    "        all_merged['Day_of_Week'] = day_of_week\n",
    "\n",
    "        all_merged['Source_File'] = os.path.basename(file_path)\n",
    "        return all_merged\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "for i, file_path in enumerate(file_paths):\n",
    "    if os.path.basename(file_path) in processed_files:\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing file {i+1}/{len(file_paths)}: {file_path}\")\n",
    "    processed_df = process_file(file_path)\n",
    "    if not processed_df.empty:\n",
    "        consolidated_df = pd.concat([consolidated_df, processed_df], ignore_index=True)\n",
    "\n",
    "    # Save periodically\n",
    "    if (i + 1) % 10 == 0:\n",
    "        consolidated_df.to_csv(consolidated_file_path, index=False)\n",
    "        print(f\"Progress saved after processing {i+1} files.\")\n",
    "\n",
    "# Final save\n",
    "consolidated_df.to_csv(consolidated_file_path, index=False)\n",
    "print(\"All files processed and saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5310d181-1c98-41f5-b7d9-081b48a3e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('New_Box_Scores.csv')\n",
    "df = df.fillna(0)\n",
    "df = df.loc[df['Name'] != '0']\n",
    "\n",
    "columns_to_encode = [\n",
    "    'Opposing Team', 'Opposing Pitcher', 'Position', 'Team', \n",
    "    'Venue', 'Conditions', 'Day_of_Week', 'Name', 'Start Time']\n",
    "\n",
    "encoders = {}\n",
    "\n",
    "for col in columns_to_encode:\n",
    "    encoders[col] = joblib.load(f'{col}_encoder.joblib')\n",
    "\n",
    "for col in columns_to_encode:\n",
    "    le = encoders[col]\n",
    "    new_classes = set(df[col].unique()) - set(le.classes_)\n",
    "    le.classes_ = np.concatenate([le.classes_, list(new_classes)])\n",
    "    df[f'{col} Encoded'] = le.transform(df[col])\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(['Name Encoded', 'Date'])\n",
    "\n",
    "excluded_columns = [\n",
    "    'Date', 'Start Time', 'Attendance', 'Duration', 'Source File', \n",
    "    'Opposing Team', 'Opposing Pitcher', 'Position', 'Team', \n",
    "    'Venue', 'Conditions', 'Day_of_Week', 'Name', \n",
    "    'Opposing Team Encoded', 'Opposing Pitcher Encoded', 'Position Encoded', \n",
    "    'Team Encoded', 'Venue Encoded', 'Conditions Encoded', 'Day_of_Week Encoded', \n",
    "    'Name Encoded', 'Start Time Encoded']\n",
    "\n",
    "numeric_columns = df.select_dtypes(include='number').columns.tolist()\n",
    "columns_to_average = [col for col in numeric_columns if col not in excluded_columns]                  #When sorting with these long strings, there is a flukey issue I've encountered within Pandas where some of the strings switch columns for whatever reason. For a more advanced model leveranging string type data, make sure you do not sort the CSV file before encoding the data \n",
    "\n",
    "for col in tqdm(columns_to_average, desc=\"Processing columns\"):\n",
    "    rolling_avg_col_name = f'{col} Last 5 Avg'\n",
    "    df[rolling_avg_col_name] = df.groupby('Name Encoded')[col].shift().rolling(window=5).mean()\n",
    "\n",
    "df = df.fillna(0)\n",
    "\n",
    "df.to_csv('New_Box_Scores_Filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b7c52-5955-4ff5-9715-9916df072240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd     #Merges Current Season Data with Past Seasons Data\n",
    "\n",
    "New_Box_Scores_Filtered = pd.read_csv('New_Box_Scores_Filtered.csv')\n",
    "box_scores_filtered = pd.read_csv('box_scores_filtered.csv')\n",
    "\n",
    "combined_df = pd.concat([New_Box_Scores_Filtered, box_scores_filtered], axis=0, ignore_index=True)\n",
    "combined_df = combined_df.drop_duplicates()\n",
    "combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]\n",
    "combined_df = combined_df.fillna(0)\n",
    "\n",
    "combined_df.to_csv('Updated_Box_Scores.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6be86-0bf0-4821-baa5-a0e17d89a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                        #This Script updates the model with the data from recent games \n",
    "from sklearn.model_selection import train_test_split       #Should be done periodically for best results\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "df = pd.read_csv('Updated_Box_Scores.csv')                    #Basic Model\n",
    "\n",
    "target_columns = ['H-A', 'BB-A', 'H+R+RBI', 'RBI', 'R', 'SO', 'SO-A', 'W/L', 'HR']\n",
    "\n",
    "feature_columns = [col for col in df.columns if col.endswith('Avg') or col in [\n",
    "    'Home/Away', 'Year', 'Month', 'Day', 'Opposing Team Encoded', \n",
    "    'Opposing Pitcher Encoded',  'Team Encoded', 'Name Encoded']]\n",
    "\n",
    "X = df[feature_columns]\n",
    "y = df[target_columns]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred, multioutput='raw_values')\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "joblib.dump(model, 'basic_mlb_player_stats_linear_model.joblib')      #Saves in place of Old Model\n",
    "print(\"Model saved to 'basic_mlb_player_stats_linear_model.joblib'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
